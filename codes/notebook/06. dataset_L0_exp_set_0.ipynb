{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- memory usage of processing sampling shuffle is *huge*\n",
    "    - subset processing\n",
    "    - using collate_fn during training\n",
    "- using summary feature as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from memory_profiler import profile\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset generated for experiment set 0 with model L0\n",
    "data_path = '../../data/dataset'\n",
    "!ls $data_path\n",
    "f1 = h5py.File(data_path + '/dataset_part_0.h5', 'r')\n",
    "f2 = h5py.File(data_path + '/dataset_part_0_snn.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1['data']['seqC'], f1['data']['theta'], f1['data']['probR']\n",
    "f2['data']['seqC'], f2['data']['theta'], f2['data']['probR']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the two datasets are the same (using same seed on different machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('comparing snn and uzh generated datasets...')\n",
    "issame_seqC = not np.sum(np.nan_to_num(f1['data']['seqC'][:], -1)!=np.nan_to_num(f2['data']['seqC'][:], -1))\n",
    "issame_theta = not np.sum(np.nan_to_num(f1['data']['theta'][:], -1)!=np.nan_to_num(f2['data']['theta'][:], -1))\n",
    "issame_probR = not np.sum(np.nan_to_num(f1['data']['probR'][:], -1)!=np.nan_to_num(f2['data']['probR'][:], -1))\n",
    "print(\"seqC are the same\") if issame_seqC else print(\"!seqC are different!\")\n",
    "print(\"theta are the same\") if issame_theta else print(\"!theta are different!\")\n",
    "print(\"probR are the same\") if issame_probR else print(\"!probR are different!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files in the data directory using python\n",
    "data_dir = '../../data/dataset'\n",
    "files = [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))]\n",
    "\n",
    "data_paths = []\n",
    "data_file_idxs = []\n",
    "for data_file in files:\n",
    "    data_path = f'{data_dir}/{data_file}'\n",
    "    data_paths.append(data_path)\n",
    "    # sort the data paths according to the file name\n",
    "    data_paths.sort(key=lambda x: x.split('_part_')[-1].split('.')[0])\n",
    "    data_file_idxs.append(int(data_path.split('_part_')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new h5 file to store the combined dataset\n",
    "merged_data_path = f'{data_dir}/dataset_L0_exp_set_0.h5'\n",
    "\n",
    "with h5py.File(merged_data_path, 'w') as merged_data_file:\n",
    "    \n",
    "    for data_path, data_file_idx in zip(*(data_paths, data_file_idxs)):\n",
    "        print(f'adding {data_path} to {merged_data_path}')\n",
    "        data_file = h5py.File(data_path, 'r')\n",
    "        # copy the data group from the data file to the merged data file\n",
    "        set_group = merged_data_file.create_group(f'set_{data_file_idx}')\n",
    "        data_file.copy('data/seqC', set_group)\n",
    "        data_file.copy('data/probR', set_group)\n",
    "        data_file.copy('data/theta', set_group)\n",
    "        data_file.close()\n",
    "        merged_data_file.flush()\n",
    "\n",
    "    print(f'\\nmerged file has keys: {list(merged_data_file.keys())}')    \n",
    "    print(f\"\\nin one set_0 it has keys: {list(merged_data_file['set_0'].keys())}\")\n",
    "    print(f\"seqC has a shape of: {merged_data_file['set_0']['seqC'].shape}\")\n",
    "    print(f\"theta has a shape of: {merged_data_file['set_0']['theta'].shape}\")\n",
    "    print(f\"probR has a shape of: {merged_data_file['set_0']['probR'].shape}\")\n",
    "\n",
    "# merged_data_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process the x with MEM check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile(precision=4)\n",
    "def load_dataset():    \n",
    "    data_path = '../../data/dataset/dataset_L0_exp_set_0.h5'\n",
    "    f = h5py.File(data_path, 'r')\n",
    "    seqC = f['set_0']['seqC'][:]\n",
    "    probR = f['set_0']['probR'][:]\n",
    "    print(seqC.shape, probR.shape)\n",
    "    seqC = seqC.reshape((seqC.shape[0]*seqC.shape[1]*seqC.shape[2], seqC.shape[3]))\n",
    "    probR = probR.reshape((probR.shape[0]*probR.shape[1]*probR.shape[2], probR.shape[3]))\n",
    "    print(seqC.shape, probR.shape)\n",
    "    return seqC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_1584372/3769736698.py\n",
      "(7, 3, 700, 15) (7, 3, 700, 5000, 1)\n",
      "(14700, 15) (14700, 5000)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/wehe/anaconda3/envs/sbi/lib/python3.7/site-packages/memory_profiler.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurences   Line Contents\n",
      "============================================================\n",
      "  1140    338.2 MiB    338.2 MiB           1               def wrapper(*args, **kwargs):\n",
      "  1141    338.2 MiB      0.0 MiB           1                   prof = get_prof()\n",
      "  1142    340.3 MiB      2.1 MiB           1                   val = prof(func)(*args, **kwargs)\n",
      "  1143    340.3 MiB      0.0 MiB           1                   show_results_bound(prof)\n",
      "  1144    340.3 MiB      0.0 MiB           1                   return val"
     ]
    }
   ],
   "source": [
    "%%mprun -f load_dataset\n",
    "\n",
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reshaped dataset\n",
    "data_path = '../../data/dataset/dataset_L0_exp_set_0.h5'\n",
    "f = h5py.File(data_path, 'r')\n",
    "seqC = f['set_0']['seqC'][:]\n",
    "probR = f['set_0']['probR'][:]\n",
    "print(seqC.shape, probR.shape)\n",
    "seqC_ = seqC.reshape((seqC.shape[0]*seqC.shape[1]*seqC.shape[2], seqC.shape[3]))\n",
    "probR_ = probR.reshape((probR.shape[0]*probR.shape[1]*probR.shape[2], probR.shape[3]))\n",
    "print(seqC_.shape, probR_.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset and dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4])\n",
      "torch.Size([10, 10, 4]) torch.Size([10, 10, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wehe/anaconda3/envs/sbi/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "probR = [0.5, 0.3, 0.1, 0.1]\n",
    "\n",
    "probR    = torch.tensor(probR).unsqueeze_(dim=0).repeat_interleave(10, dim=0) # (C, D*M*S, 1)\n",
    "x_choice = torch.bernoulli(probR) # (C, D*M*S, 1)\n",
    "print(x_choice.shape, probR.shape)\n",
    "probR = torch.tensor(probR).expand(10, -1, -1)\n",
    "x_choice = torch.bernoulli(probR) # (C, D*M*S, 1)\n",
    "print(x_choice.shape, probR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from train.MyData import MyDataset\n",
    "\n",
    "class MyDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, C, shuffle=True, pin_memory=False, num_workers=0):\n",
    "        super().__init__(dataset, batch_size, shuffle=shuffle, collate_fn=self.collate_fn, pin_memory=pin_memory, num_workers=num_workers)\n",
    "        self.C = C\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        # Process the batch\n",
    "        x_batches, theta_batches = [], []\n",
    "        \n",
    "        # x_batch     = torch.empty((self.C * len(batch), batch[0][0].shape[0], batch[0][0].shape[1]+batch[0][2].shape[1]))\n",
    "        # theta_batch = torch.empty((self.C * len(batch), batch[0][1].shape[0]))\n",
    "        # Create tensors for probR and x_seqC outside the loop\n",
    "        x_seqC_tensor = torch.empty((self.C, batch[0][0].shape[0], batch[0][0].shape[1]))\n",
    "        probR_tensor = torch.empty((self.C, batch[0][2].shape[0], 1))\n",
    "        print(len(batch), x_seqC_tensor.shape, probR_tensor.shape)\n",
    "        \n",
    "        for i, (seqC, theta, probR) in enumerate(batch): # seqC: (D*M*S, 15), theta: (4,), probR: (D*M*S, 1)\n",
    "            \n",
    "            # probR    = torch.tensor(probR).unsqueeze_(dim=0).repeat_interleave(self.C, dim=0) # (C, D*M*S, 1)\n",
    "            probR_tensor.copy_(torch.tensor(probR).unsqueeze_(dim=0).repeat_interleave(self.C, dim=0)) # (C, D*M*S, 1)\n",
    "            x_choice = torch.bernoulli(probR_tensor) # (C, D*M*S, 1)\n",
    "            \n",
    "            # x_seqC   = torch.tensor(seqC).unsqueeze_(dim=0).repeat_interleave(self.C, dim=0) # (C, D*M*S, 15)\n",
    "            # x = torch.cat([x_seqC, x_choice], dim=-1)\n",
    "            x_seqC_tensor.copy_(torch.tensor(seqC).unsqueeze_(dim=0).repeat_interleave(self.C, dim=0)) # (C, D*M*S, 15)\n",
    "            x = torch.cat([x_seqC_tensor, x_choice], dim=-1)\n",
    "            \n",
    "            theta = torch.tensor(theta).unsqueeze_(dim=0).repeat_interleave(self.C, dim=0) # (C, 4)\n",
    "            \n",
    "            x_batches.append(x)\n",
    "            theta_batches.append(theta)\n",
    "        \n",
    "        # Concatenate the batched tensors\n",
    "        x_batch = torch.cat(x_batches, dim=0)\n",
    "        theta_batch = torch.cat(theta_batches, dim=0)\n",
    "        \n",
    "        # Shuffle x along the 2nd axis\n",
    "        # for i in range(x_batch.shape[0]):\n",
    "        #     indices = torch.randperm(x_batch.shape[1])\n",
    "        #     x_batch[i] = x_batch[i][indices]\n",
    "        x_batch = torch.stack([x_batch[i][torch.randperm(x_batch.shape[1])] for i in range(x_batch.shape[0])])\n",
    "            \n",
    "        # Shuffle the batched dataset\n",
    "        indices     = torch.randperm(x_batch.shape[0])\n",
    "        x_batch     = x_batch[indices]\n",
    "        theta_batch = theta_batch[indices]\n",
    "        \n",
    "        return torch.tensor(x_batch, dtype=torch.float32), torch.tensor(theta_batch, dtype=torch.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "data_path = \"../../data/dataset/dataset_L0_exp_set_0.h5\"\n",
    "batch_size = 200\n",
    "C = 100\n",
    "\n",
    "# Create a my dataset using the data path\n",
    "my_dataset = MyDataset(data_path)\n",
    "\n",
    "# Create a my data loader using the dataset, batch size, and C\n",
    "my_dataloader = MyDataLoader(my_dataset, batch_size, C, shuffle=False)\n",
    "\n",
    "@profile\n",
    "def load_my_dataset():    \n",
    "    next(iter(my_dataloader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_1591563/4076220988.py\n",
      "200 torch.Size([100, 14700, 15]) torch.Size([100, 14700, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wehe/anaconda3/envs/sbi/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/wehe/anaconda3/envs/sbi/lib/python3.7/site-packages/memory_profiler.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurences   Line Contents\n",
      "============================================================\n",
      "  1140    328.7 MiB    328.7 MiB           1               def wrapper(*args, **kwargs):\n",
      "  1141    328.7 MiB      0.0 MiB           1                   prof = get_prof()\n",
      "  1142  18648.8 MiB  18320.1 MiB           1                   val = prof(func)(*args, **kwargs)\n",
      "  1143  18648.8 MiB      0.0 MiB           1                   show_results_bound(prof)\n",
      "  1144  18648.8 MiB      0.0 MiB           1                   return val"
     ]
    }
   ],
   "source": [
    "%%mprun -f load_my_dataset\n",
    "\n",
    "load_my_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(my_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2500, 14700, 16]), torch.Size([2500, 4]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape, data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
