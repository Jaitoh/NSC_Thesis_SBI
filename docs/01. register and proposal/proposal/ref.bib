
@online{PlaceTheorySound,
  title = {A Place Theory of Sound Localization. - {{PsycNET}}},
  url = {https://doi.apa.org/doiLanding?doi=10.1037%2Fh0061495},
  urldate = {2022-02-19},
  abstract = {APA PsycNet DoiLanding page},
  langid = {english},
  annotation = {00000},
  year={1948}
}



@inproceedings{xuMachineHearingSystem2018,
  title = {A {{Machine Hearing System}} for {{Binaural Sound Localization}} Based on {{Instantaneous Correlation}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Xu, Ying and Afshar, Saeed and Singh, Ram Kuber and Hamilton, Tara Julia and Wang, Runchun and van Schaik, Andre},
  options = {useprefix=true},
  date = {2018},
  year = {2018},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Florence}},
  doi = {10.1109/ISCAS.2018.8351367},
  url = {https://ieeexplore.ieee.org/document/8351367/},
  urldate = {2022-01-07},
  abstract = {We propose a biologically inspired binaural sound localization system for reverberant environments. It uses two 100-channel cochlear models to analyze binaural signals, and each channel of the left cochlea is compared with each channel of the right cochlea in parallel to generate a 2-D instantaneous correlation matrix (correlogram). The correlogram encodes both binaural cues and spectral information in a unified framework. A sound onset detector is used to generate the correlogram only during the sound onsets, and the onset correlogram is analyzed using a linear regression approach as well as an extreme learning machine (ELM). The proposed system is evaluated using experimental data in reverberation environments, and we obtained an average absolute error of 16.5° for linear regression and 12.8° for ELM regression in the -90° to 90° range.},
  eventtitle = {2018 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  isbn = {978-1-5386-4881-0},
  langid = {english},
  keywords = {to read},
  annotation = {00004},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/MCFVHSAC/Xu et al 2018 - A Machine Hearing System for Binaural Sound Localization based on Instantaneous.pdf}
}



@inproceedings{fingerEstimatingLocationSound2011,
  title = {Estimating the Location of a Sound Source with a Spike-Timing Localization Algorithm},
  booktitle = {2011 {{IEEE International Symposium}} of {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Finger, Holger and Liu, Shih-Chii},
  date = {2011-05},
  year = {2011},
  pages = {2461--2464},
  publisher = {{IEEE}},
  location = {{Rio de Janeiro, Brazil}},
  doi = {10.1109/ISCAS.2011.5938102},
  url = {http://ieeexplore.ieee.org/document/5938102/},
  urldate = {2021-08-26},
  abstract = {A binaural spike-based sound localization system suitable for real-time attention systems of robots is presented in this work. This system uses the output spikes from a 64-channel binaural silicon cochlea. The localization algorithm implements a spike-based correlation of these output spikes that measures the interaural time differences (ITDs) between the arrival of a sound to two microphones. The algorithm continuously updates a possible distribution of ITDs in the auditory scene whenever a spike arrives from either ear of the cochlea. Experimental results show that the system can estimate an azimuth angle to below 1 deg resolution. This algorithm is computationally cheaper than conventional generalized cross-correlation methods and is quicker to respond to changes in the scene.},
  eventtitle = {2011 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  isbn = {978-1-4244-9473-6},
  langid = {english},
  keywords = {algor,read},
  annotation = {00020},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/NJH86NFG/Finger 和 Liu - 2011 - Estimating the location of a sound source with a s.pdf}
}




@book{lyonHumanMachineHearing2018,
  title = {Human and {{Machine Hearing}}: {{Extracting Meaning}} from {{Sound}}},
  shorttitle = {Human and {{Machine Hearing}}},
  author = {Lyon, Richard F.},
  date = {2018-01-01},
  year = {2018},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781139051699},
  url = {https://www.cambridge.org/core/product/identifier/9781139051699/type/book},
  urldate = {2022-01-19},
  isbn = {978-1-107-00753-6 978-1-139-05169-9},
  langid = {english},
  keywords = {next to read},
  annotation = {00000},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/H3GMCN5A/Lyon 2018 - Human and Machine Hearing.pdf}
}



@article{rasettoEventBasedTimeVectors2021,
  title = {Event {{Based Time-Vectors}} for Auditory Features Extraction: A Neuromorphic Approach for Low Power Audio Recognition},
  shorttitle = {Event {{Based Time-Vectors}} for Auditory Features Extraction},
  author = {Rasetto, Marco and Dominguez-Morales, Juan P. and Jimenez-Fernandez, Angel and Benosman, Ryad},
  date = {2021-12-13},
  year = {2021},
  eprint = {2112.07011},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2112.07011},
  urldate = {2022-02-17},
  abstract = {In recent years tremendous efforts have been done to advance the state of the art for Natural Language Processing (NLP) and audio recognition. However, these efforts often translated in increased power consumption and memory requirements for bigger and more complex models. These solutions falls short of the constraints of IoT devices which need low power, low memory efficient computation, and therefore they fail to meet the growing demand of efficient edge computing. Neuromorphic systems have proved to be excellent candidates for low-power low-latency computation in a multitude of applications. For this reason we present a neuromorphic architecture, capable of unsupervised auditory feature recognition. We then validate the network on a subset of Google’s Speech Commands dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {read},
  annotation = {00000},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/M7VFFC9D/Rasetto et al. - 2021 - Event Based Time-Vectors for auditory features ext.pdf}
}



@article{lyonUsingCascadeAsymmetric2011,
  title = {Using a {{Cascade}} of {{Asymmetric Resonators}} with {{Fast-Acting Compression}} as a {{Cochlear Model}} for {{Machine-Hearing Applications}}},
  author = {Lyon, Richard F},
  date = {2011},
  year = {2011},
  pages = {4},
  abstract = {Every day, machines process many thousands of hours of audio signals through a realistic cochlear model. They extract features, inform classifiers and recommenders, and identify copyrighted material. The machine-hearing approach to such tasks has taken root in recent years, because hearingbased approaches perform better than we can do with more conventional sound-analysis approaches. We use a bio-mimetic “cascade of asymmetric resonators with fast-acting compression” (CARFAC)—an efficient sound analyzer that incorporates the hearing research community’s findings on nonlinear auditory filter models and cochlear wave mechanics. The CAR-FAC is based on a pole-zero filter cascade (PZFC) model of auditory filtering, in combination with a multi-time-scale coupled automaticgain-control (AGC) network. It uses simple nonlinear extensions of conventional digital filter stages, and runs fast due to its low complexity. The PZFC plus AGC network, the CAR-FAC, mimics features of auditory physiology, such as masking, compressive traveling-wave response, and the stability of zero-crossing times with signal level. Its output “neural activity pattern” is converted to a “stabilized auditory image” to capture pitch, melody, and other temporal and spectral features of the sound.},
  langid = {english},
  keywords = {read},
  annotation = {00011},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/J2VKMX8H/Lyon 2011 - Using a Cascade of Asymmetric Resonators with Fast-Acting Compression as a Cochlear Model for Machine-Hearing Applications.pdf}
}

@article{thakurFPGA_CAR2014,
  title = {{{FPGA}} Implementation of the {{CAR Model}} of the Cochlea},
  booktitle = {2014 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Thakur, Chetan Singh and Hamilton, Tara Julia and Tapson, Jonathan and van Schaik, Andre and Lyon, Richard F.},
  options = {useprefix=true},
  date = {2014-06},
  year = {2014},
  pages = {1853--1856},
  publisher = {{IEEE}},
  location = {{Melbourne VIC, Australia}},
  doi = {10.1109/ISCAS.2014.6865519},
  url = {http://ieeexplore.ieee.org/document/6865519/},
  urldate = {2022-01-13},
  abstract = {The front end of the human auditory system, the cochlea, converts sound signals from the outside world into neural impulses transmitted along the auditory pathway for further processing. The cochlea senses and separates sound in a nonlinear active fashion, exhibiting remarkable sensitivity and frequency discrimination. Although several electronic models of the cochlea have been proposed and implemented, none of these are able to reproduce all the characteristics of the cochlea, including large dynamic range, large gain and sharp tuning at low sound levels, and low gain and broad tuning at intense sound levels. Here, we implement the ‘Cascade of Asymmetric Resonators’ (CAR) model of the cochlea on an FPGA. CAR represents the basilar membrane filter in the ‘Cascade of Asymmetric Resonators with Fast-Acting Compression’ (CARFAC) cochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zero filter cascade model of auditory filtering. It uses simple nonlinear extensions of conventional digital filter stages that are well suited to FPGA implementations, so that we are able to implement up to 1224 cochlear sections on Virtex-6 FPGA to process sound data in real time. The FPGA implementation of the electronic cochlea described here may be used as a front-end sound analyser for various machine-hearing applications.},
  eventtitle = {2014 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  isbn = {978-1-4799-3432-4 978-1-4799-3431-7},
  langid = {english},
  keywords = {read},
  annotation = {00024},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/YIT95NYU/Thakur et al 2014 - FPGA implementation of the CAR Model of the cochlea.pdf}
}

@article{xuBioSoundLocalisation2021,
  title = {A {{Biologically Inspired Sound Localisation System Using}} a {{Silicon Cochlea Pair}}},
  author = {Xu, Ying and Afshar, Saeed and Wang, Runchun and Cohen, Gregory and Singh Thakur, Chetan and Hamilton, Tara Julia and van Schaik, André},
  options = {useprefix=true},
  date = {2021-02-08},
  year = {2021},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {11},
  number = {4},
  pages = {1519},
  issn = {2076-3417},
  doi = {10.3390/app11041519},
  url = {https://www.mdpi.com/2076-3417/11/4/1519},
  urldate = {2021-12-21},
  abstract = {We present a biologically inspired sound localisation system for reverberant environments using the Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC) cochlear model. The system exploits a CAR-FAC pair to pre-process binaural signals that travel through the inherent delay line of the cascade structures, as each filter acts as a delay unit. Following the filtering, each cochlear channel is cross-correlated with all the channels of the other cochlea using a quantised instantaneous correlation function to form a 2-D instantaneous correlation matrix (correlogram). The correlogram contains both interaural time difference and spectral information. The generated correlograms are analysed using a regression neural network for localisation. We investigate the effect of the CAR-FAC nonlinearity on the system performance by comparing it with a CAR only version. To verify that the CAR/CAR-FAC and the quantised instantaneous correlation provide a suitable basis with which to perform sound localisation tasks, a linear regression, an extreme learning machine, and a convolutional neural network are trained to learn the azimuthal angle of the sound source from the correlogram. The system is evaluated using speech data recorded in a reverberant environment. We compare the performance of the linear CAR and nonlinear CAR-FAC models with current sound localisation systems as well as with human performance.},
  langid = {english},
  keywords = {algor,hw,read},
  annotation = {00000},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/26EK7BKI/Xu et al 2021 - A Biologically Inspired Sound Localisation System Using a Silicon Cochlea Pair.pdf}
}

@article{xuFPGA_CARFAC2018,
  title = {A {{FPGA Implementation}} of the {{CAR-FAC Cochlear Model}}},
  author = {Xu, Ying and Thakur, Chetan S. and Singh, Ram K. and Hamilton, Tara Julia and Wang, Runchun M. and van Schaik, André},
  options = {useprefix=true},
  date = {2018-04-10},
  year = {2018},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {12},
  pages = {198},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00198},
  url = {http://journal.frontiersin.org/article/10.3389/fnins.2018.00198/full},
  urldate = {2022-01-07},
  abstract = {This paper presents a digital implementation of the Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC) cochlear model. The CAR part simulates the basilar membrane’s (BM) response to sound. The FAC part models the outer hair cell (OHC), the inner hair cell (IHC), and the medial olivocochlear efferent system functions. The FAC feeds back to the CAR by moving the poles and zeros of the CAR resonators automatically. We have implemented a 70-section, 44.1 kHz sampling rate CAR-FAC system on an Altera Cyclone V Field Programmable Gate Array (FPGA) with 18\% ALM utilization by using time-multiplexing and pipeline parallelizing techniques and present measurement results here. The fully digital reconfigurable CAR-FAC system is stable, scalable, easy to use, and provides an excellent input stage to more complex machine hearing tasks such as sound localization, sound segregation, speech recognition, and so on.},
  langid = {english},
  keywords = {algor,hw,read},
  file = {/Users/admin/OneDrive - ETH Zurich/Zotero_HV16/storage/XYYJEJJ6/Xu et al 2018 - A FPGA Implementation of the CAR-FAC Cochlear Model.pdf}
}


